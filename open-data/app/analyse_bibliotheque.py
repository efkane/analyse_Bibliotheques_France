import os
from attr import s
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import random
from haversine import haversine  
import requests
from bs4 import BeautifulSoup
import json
import pulp
import community as community_louvain
import community.community_louvain as community_louvain
import matplotlib.pyplot as plt
import numpy as np
import pulp
import streamlit
import streamlit as st
import folium
from streamlit_folium import folium_static

from pyvis.network import Network
from pulp import PULP_CBC_CMD, LpStatus
import os
import pandas as pd
import networkx as nx
import folium
import pulp
import streamlit as st
import plotly.express as px
from haversine import haversine
from streamlit_folium import st_folium
from folium.plugins import MarkerCluster
from pyvis.network import Network

import networkx as nx
import matplotlib.pyplot as plt
import pulp
from community import community_louvain
import streamlit as st

# ðŸ“ Importation du fichier contenant les fonctions de graphe et chemin
import graphe_ouvrages

# ===============================
# ðŸ”² MENU LATERAL DE NAVIGATION
# ===============================

choix = st.sidebar.radio(
    "ðŸ”Ž Choisir une visualisation :",
    ["RÃ©seau gÃ©ographique", "RÃ©seau par ouvrages", "Chemin optimal"]
)

# ===============================
# ðŸŒ GRAPHE CLASSIQUE PAR DISTANCE
# ===============================
if choix == "RÃ©seau gÃ©ographique":

    st.write("ðŸŒ RÃ©seaux Interactifs Des BibliothÃ¨ques En")

# ===============================
# ðŸ“š GRAPHE PAR OUVRAGES DIFFÃ‰RENTS
# ===============================
elif choix == "RÃ©seau par ouvrages":
    graphe_ouvrages.afficher_graphe()

# ===============================
# ðŸ§­ CHEMIN OPTIMAL POUR LIRE TOUS LES LIVRES
# ===============================
elif choix == "Chemin optimal":
    graphe_ouvrages.afficher_chemin_optimal()


# ðŸ“Œ Ã‰tape 1 : Chargement et Nettoyage des DonnÃ©es

fichier_csv = "data/bibliotheques.csv"
df = pd.read_csv(fichier_csv, encoding="utf-8", delimiter=";")

df = df.dropna(subset=["Latitude", "Longitude", "Nom de l'Ã©tablissement"])
df["Latitude"] = df["Latitude"].astype(float)
df["Longitude"] = df["Longitude"].astype(float)

# RÃ©duction de la taille pour Ã©viter les lenteurs
df = df.sample(n=200, random_state=42)  # au lieu de tout charger

print(f"âœ… Nombre total de bibliothÃ¨ques sÃ©lectionnÃ©es : {len(df)}")

print("âœ… Ã‰tape 1 terminÃ©e : DonnÃ©es chargÃ©es et nettoyÃ©es")
# =====================================================
# ðŸ“Œ Ã‰TAPE 2 DU PROJET :  Manipulation de graphes avec networkx
# =====================================================

# ðŸ“Œ Ã‰tape 2.1 : CrÃ©ation du Graphe avec NetworkX

G = nx.Graph()

for _, row in df.iterrows():
    G.add_node(row["Nom de l'Ã©tablissement"], 
               ville=row["Ville"], 
               region=row["RÃ©gion"], 
               latitude=row["Latitude"], 
               longitude=row["Longitude"], 
               surface=row["Surface"])

# Seuil de distance et limitation des arÃªtes
seuil_distance_km = 100
max_edges = 5000
edge_count = 0

bibliotheques = list(G.nodes(data=True))

for i in range(len(bibliotheques)):
    for j in range(i + 1, len(bibliotheques)):
        if edge_count >= max_edges:
            break  

        biblio1 = bibliotheques[i]
        biblio2 = bibliotheques[j]

        coord1 = (biblio1[1]["latitude"], biblio1[1]["longitude"])
        coord2 = (biblio2[1]["latitude"], biblio2[1]["longitude"])
        distance = haversine(coord1, coord2)

        if distance < seuil_distance_km:
            G.add_edge(biblio1[0], biblio2[0], weight=distance)
            edge_count += 1

print(f"âœ… Nombre total d'arÃªtes aprÃ¨s limitation : {G.number_of_edges()}")
print("âœ… Ã‰tape 2 terminÃ©e : Graphe crÃ©Ã©")

# ðŸ“Œ Ã‰tape 2.2 : Visualisation du Graphe

print("âž¡ DÃ©but de l'Ã©tape 3 : Visualisation du graphe")

# RÃ©duire l'affichage Ã  un sous-graphe de 200 bibliothÃ¨ques max
if G.number_of_nodes() > 200:
    subgraph_nodes = list(G.nodes)[:200]
    G_subgraph = G.subgraph(subgraph_nodes)
    print("âš  Affichage d'un sous-graphe de 200 bibliothÃ¨ques au lieu de tout le graphe")
else:
    G_subgraph = G

# Affichage du graphe recentrÃ©
pos = nx.spring_layout(G_subgraph, k=0.3)

plt.figure(figsize=(12, 8))
nx.draw_networkx_edges(G_subgraph, pos, alpha=0.3, edge_color="gray")
nx.draw_networkx_nodes(G_subgraph, pos, node_size=50, node_color="blue")

plt.title("Graphe des bibliothÃ¨ques")
plt.show(block=False)
plt.pause(3)
plt.close()

print("âœ… Ã‰tape 3 terminÃ©e : Graphe affichÃ©")

# ðŸ“Œ Ã‰tape 2.3 : Affichage des Sommets et des ArÃªtes

print("\nðŸ“Œ Liste des bibliothÃ¨ques dans le graphe (10 premiÃ¨res affichÃ©es) :")
bibliotheques_list = list(G.nodes())[:10]
print(bibliotheques_list)
if len(G.nodes()) > 10:
    print("... (", len(G.nodes()) - 10, "bibliothÃ¨ques supplÃ©mentaires non affichÃ©es)")

print("\nðŸ“Œ Liste des connexions (arÃªtes) entre bibliothÃ¨ques (10 premiÃ¨res affichÃ©es) :")
edges_list = list(G.edges())[:10]
print(edges_list)
if len(G.edges()) > 10:
    print("... (", len(G.edges()) - 10, "connexions supplÃ©mentaires non affichÃ©es)")

print(f"\nâœ… Le graphe contient {G.number_of_nodes()} bibliothÃ¨ques.")
print(f"âœ… Il y a {G.number_of_edges()} connexions entre elles.")

# ðŸ“Œ Ã‰tape 2.4: Analyse du Graphe (BibliothÃ¨ques Centrales)

degree_centrality = nx.degree_centrality(G)
top_bibliotheques = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

print("\nðŸ“Œ Top 10 des bibliothÃ¨ques les plus connectÃ©es :")
for bibliotheque, degre in top_bibliotheques:
    print(f"{bibliotheque} - Score de centralitÃ© : {degre:.3f}")

print("âœ… Ã‰tape 2.4 terminÃ©e : Analyse des bibliothÃ¨ques centrales.")

# ðŸ“Œ Ã‰tape 2.5 : GÃ©nÃ©ration de Graphes de Familles Connues

n = 10  
p = 0.3  

G_erdos = nx.erdos_renyi_graph(n, p)
G_barabasi = nx.barabasi_albert_graph(n=10, m=2)
G_bipartite = nx.complete_bipartite_graph(5, 5)
G_cycle = nx.cycle_graph(6)

print("\nðŸ“Œ Exemples de graphes gÃ©nÃ©rÃ©s :")
print("G_erdos:", G_erdos.number_of_nodes(), "nÅ“uds,", G_erdos.number_of_edges(), "arÃªtes.")
print("G_barabasi:", G_barabasi.number_of_nodes(), "nÅ“uds,", G_barabasi.number_of_edges(), "arÃªtes.")
print("G_bipartite:", G_bipartite.number_of_nodes(), "nÅ“uds,", G_bipartite.number_of_edges(), "arÃªtes.")
print("G_cycle:", G_cycle.number_of_nodes(), "nÅ“uds,", G_cycle.number_of_edges(), "arÃªtes.")

print("âœ… Ã‰tape 2.5 terminÃ©e : GÃ©nÃ©ration de graphes terminÃ©e.")

# ðŸ“Œ Ã‰tape 2.6 : Chargement et Visualisation dâ€™un Graphe depuis un Fichier

nom_fichier = "test.txt"

if not os.path.exists(nom_fichier):
    print(f"âš  Erreur : Le fichier '{nom_fichier}' n'existe pas !")
    print("ðŸ‘‰ VÃ©rifie le nom du fichier ou crÃ©e un fichier test.txt dans le bon dossier.")
else:
    def charger_graphe(source):
        G = nx.Graph()
        with open(source, 'r', encoding='utf-8') as fichier:
            fichier.readline()
            contenu = fichier.read()
            liste_couples = contenu.split("\n")
            for couple in liste_couples:
                if couple.strip() == "":
                    continue
                couple_etudiants = couple.split(";")
                if len(couple_etudiants) == 2:
                    G.add_edge(couple_etudiants[0], couple_etudiants[1])
        return G

    G_fichier = charger_graphe(nom_fichier)

    plt.figure(figsize=(10, 6))
    nx.draw_shell(G_fichier, with_labels=False, node_size=50, node_color="blue", edge_color="gray")
    plt.savefig('plotgraph.png', dpi=300)
    plt.show(block=False)
    plt.pause(3)
    plt.close()

    print("âœ… Ã‰tape 2.6 terminÃ©e : Chargement et visualisation du graphe depuis un fichier rÃ©ussis.")
# =====================================================
# ðŸ“Œ Ã‰TAPE 4 DU PROJET : RÃ©cupÃ©ration dâ€™informations (Scraping, APIs)
# =====================================================

# ðŸ“Œ **4.1 - Scraping de donnÃ©es Web**
print("âž¡ DÃ©but de l'Ã©tape 4.1 : Scraping des donnÃ©es Web")

url = "https://example.com/"  # Remplace par un site rÃ©el
response = requests.get(url)

if response.status_code == 200:
    soup = BeautifulSoup(response.text, 'lxml')
    all_links = soup.find_all("a")  # RÃ©cupÃ©rer tous les liens
    links_data = [{"Texte": link.text, "URL": link.get('href')} for link in all_links[:5]]  # LimitÃ© Ã  5 liens
    print("âœ… Scraping terminÃ© : 5 premiers liens rÃ©cupÃ©rÃ©s")
else:
    print(f"âš  Ã‰chec du scraping, code d'erreur : {response.status_code}")

# ðŸ“Œ **4.2 - RequÃªtes API**
print("âž¡ DÃ©but de l'Ã©tape 4.2 : RÃ©cupÃ©ration des donnÃ©es via API")

api_url = "https://jsonplaceholder.typicode.com/posts"
response = requests.get(api_url)

if response.status_code == 200:
    articles_json = response.json()
    with open("data/articles.json", "w", encoding='utf-8') as f:
        json.dump(articles_json, f, ensure_ascii=False, indent=2)
    print("âœ… API accessible, donnÃ©es sauvegardÃ©es dans articles.json")
else:
    print(f"âš  Erreur API : {response.status_code}")


# ðŸ“Œ **4.3 - Recherche sur Yahoo**
print("âž¡ DÃ©but de l'Ã©tape 4.3 : Recherche Yahoo")

def requete_yahoo(chaine):
    headers = {
        'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                      '(KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582'
    }
    params = {'p': chaine}
    
    response = requests.get('https://fr.search.yahoo.com/search', headers=headers, params=params)
    
    if "Nous n'avons trouvÃ© aucun rÃ©sultat" in response.text:
        print(f"ðŸ” Recherche Yahoo '{chaine}': Aucun rÃ©sultat trouvÃ©")
        return 0
    else:
        print(f"ðŸ” Recherche Yahoo '{chaine}': Des rÃ©sultats ont Ã©tÃ© trouvÃ©s")
        return 1  # Remplacer par un vrai comptage si besoin

requete_yahoo("bibliothÃ¨ques")

print("âœ… Ã‰tape 4 terminÃ©e : Scraping et rÃ©cupÃ©ration dâ€™informations effectuÃ©s")


# =====================================================
# ðŸ“Œ Ã‰TAPE 5 DU PROJET : Formats de donnÃ©es (CSV, JSON, XLSX)
# =====================================================

# ðŸ“Œ **5.1 - Sauvegarde et chargement au format CSV**
print("âž¡ DÃ©but de l'Ã©tape 5.1 : Sauvegarde des donnÃ©es en CSV")

# Convertir les nÅ“uds en DataFrame
nodes_data = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient="index").reset_index()
nodes_data.rename(columns={"index": "Nom de l'Ã©tablissement"}, inplace=True)

# Sauvegarde en CSV
nodes_data.to_csv("bibliotheques_graph.csv", index=False)
print("âœ… DonnÃ©es du graphe sauvegardÃ©es dans bibliotheques_graph.csv")

# ðŸ“Œ **5.2 - Sauvegarde et chargement au format JSON**
print("âž¡ DÃ©but de l'Ã©tape 5.2 : Sauvegarde des donnÃ©es en JSON")

# Sauvegarde du graphe en JSON
with open("data/bibliotheques_graph.json", "w", encoding="utf-8") as f:
    json.dump(nx.node_link_data(G), f, ensure_ascii=False, indent=2)

print("âœ… DonnÃ©es du graphe sauvegardÃ©es dans bibliotheques_graph.json")

# ðŸ“Œ **5.3 - Sauvegarde et chargement au format Excel (XLSX)**
print("âž¡ DÃ©but de l'Ã©tape 5.3 : Sauvegarde des donnÃ©es en XLSX")

# Sauvegarde en Excel
nodes_data.to_excel("bibliotheques_graph.xlsx", sheet_name="BibliothÃ¨ques", index=False)

print("âœ… DonnÃ©es du graphe sauvegardÃ©es dans bibliotheques_graph.xlsx")

# ðŸ“Œ **5.4 - Export du graphe pour Gephi (GEXF)**
print("âž¡ DÃ©but de l'Ã©tape 5.4 : Export du graphe pour Gephi")

nx.write_gexf(G, "bibliotheques_graph.gexf")
print("âœ… Export du graphe terminÃ© : bibliotheques_graph.gexf")




import networkx as nx
import matplotlib.pyplot as plt
import pulp
from community import community_louvain
import streamlit as st

# Assurez-vous que le graphe 'G' est dÃ©jÃ  dÃ©fini au prÃ©alable
# Exemple : G = nx.erdos_renyi_graph(100, 0.05) ou un autre graphe

# =====================================================
# ðŸ“Œ Ã‰TAPE 7 : DÃ©tection des communautÃ©s avec Louvain
# =====================================================
st.subheader("ðŸŒ DÃ©tection des communautÃ©s avec Louvain")

# Application de l'algorithme de Louvain
partition = community_louvain.best_partition(G)
num_communities = len(set(partition.values()))
st.write(f"âœ… Nombre de communautÃ©s trouvÃ©es : {num_communities}")

# Ajout des communautÃ©s comme attributs des nÅ“uds
for node, comm_id in partition.items():
    G.nodes[node]['community'] = comm_id

# Visualisation des communautÃ©s avec Streamlit
pos = nx.spring_layout(G, k=0.3)  # Position des nÅ“uds
colors = [partition[node] for node in G.nodes()]  # Attribution des couleurs
cmap = plt.cm.viridis  # Palette de couleurs

plt.figure(figsize=(12, 8))
nx.draw_networkx_edges(G, pos, alpha=0.2, edge_color="gray")
sc = nx.draw_networkx_nodes(G, pos, node_size=50, node_color=colors, cmap=cmap, alpha=0.8)

# Ajout de la lÃ©gende
plt.colorbar(sc, label="CommunautÃ© ID")
plt.title("DÃ©tection des communautÃ©s avec l'algorithme de Louvain")

# Affichage avec Streamlit
st.pyplot(plt)

# =====================================================
# ðŸ“Œ Ã‰TAPE 8 : ExpÃ©rimentation et rÃ©glage des distances et poids
# =====================================================
st.subheader("ðŸ” ExpÃ©rimentation et rÃ©glage des distances et poids")

# Tester plusieurs seuils de distance et observer l'impact
distances_a_tester = [50, 100, 150, 200]  # Seuils de distance en km

for seuil in distances_a_tester:
    G_temp = nx.Graph()
    edge_count = 0

    for i in range(len(bibliotheques)):
        for j in range(i + 1, len(bibliotheques)):
            if edge_count >= max_edges:
                break  
            
            biblio1 = bibliotheques[i]
            biblio2 = bibliotheques[j]
            coord1 = (biblio1[1]["latitude"], biblio1[1]["longitude"])
            coord2 = (biblio2[1]["latitude"], biblio2[1]["longitude"])
            distance = haversine(coord1, coord2)

            if distance < seuil:
                G_temp.add_edge(biblio1[0], biblio2[0], weight=distance)
                edge_count += 1

    st.write(f"ðŸ“Œ Seuil de distance : {seuil} km â†’ Nombre d'arÃªtes : {G_temp.number_of_edges()}")

# =====================================================
# ðŸ“Œ Ã‰tape 9.1 : RÃ©duction du graphe avant optimisation
# =====================================================
st.subheader("ðŸ“‰ RÃ©duction du graphe avant optimisation")

# SÃ©lection des 200 bibliothÃ¨ques les plus connectÃ©es
top_200_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:200]

# CrÃ©ation dâ€™un sous-graphe avec les bibliothÃ¨ques les plus importantes
G_reduit = G.subgraph(top_200_nodes).copy()

st.write(f"âœ… Nouveau graphe pour optimisation : {G_reduit.number_of_nodes()} nÅ“uds, {G_reduit.number_of_edges()} arÃªtes.")

# =====================================================
# ðŸ“Œ Ã‰TAPE 9.2 : Optimisation avancÃ©e - Minimum Dominating Set
# =====================================================
st.subheader("âš™ï¸ Optimisation du Minimum Dominating Set")

# Liste des nÅ“uds du graphe rÃ©duit
nodes = list(G_reduit.nodes())

# DÃ©finition des variables binaires
x = pulp.LpVariable.dicts("x", nodes, cat=pulp.LpBinary)

# CrÃ©ation du problÃ¨me d'optimisation
problem = pulp.LpProblem("Minimum_Dominating_Set", pulp.LpMinimize)

# Objectif : minimiser le nombre de bibliothÃ¨ques sÃ©lectionnÃ©es
problem += pulp.lpSum([x[i] for i in nodes])

# Contrainte de domination : chaque bibliothÃ¨que doit Ãªtre couverte
for i in nodes:
    neighbors = list(G_reduit.neighbors(i)) + [i]  # Ajouter la bibliothÃ¨que elle-mÃªme
    problem += pulp.lpSum([x[n] for n in neighbors]) >= 1

st.write("ðŸ“Œ Optimisation en cours...")

# RÃ©solution du problÃ¨me avec gestion des erreurs
try:
    problem.solve()
    if pulp.LpStatus[problem.status] == "Optimal":
        dominating_set = [i for i in nodes if pulp.value(x[i]) == 1]
        st.write(f"âœ… Nombre de bibliothÃ¨ques sÃ©lectionnÃ©es pour couvrir tout le rÃ©seau : {len(dominating_set)}")
        st.write(f"ðŸ“Œ BibliothÃ¨ques sÃ©lectionnÃ©es : {dominating_set[:10]} ...")
    else:
        st.write(f"âš  ProblÃ¨me non rÃ©solu, statut : {pulp.LpStatus[problem.status]}")
except Exception as e:
    st.write(f"âŒ Erreur lors de la rÃ©solution : {e}")

# Visualisation du Minimum Dominating Set
pos = nx.spring_layout(G_reduit, k=0.3)

plt.figure(figsize=(12, 8))
nx.draw_networkx_edges(G_reduit, pos, alpha=0.3, edge_color="gray")
nx.draw_networkx_nodes(G_reduit, pos, node_size=50, node_color="blue", alpha=0.5)
nx.draw_networkx_nodes(G_reduit, pos, nodelist=dominating_set, node_size=100, node_color="red", alpha=1)

plt.title("Minimum Dominating Set Connexe")

# Affichage avec Streamlit
st.pyplot(plt)

# =====================================================
# ðŸ“Œ Ã‰TAPE 10 : ProblÃ©matique avancÃ©e : Minimum Dominating Set Connexe
# =====================================================
st.subheader("ðŸ”§ Minimum Dominating Set Connexe")

# RÃ©duction du graphe pour tester si trop lent
if G.number_of_nodes() > 200:
    sub_nodes = list(G.nodes())[:200]  # Prend un sous-ensemble de 200 nÅ“uds
    G_reduit = G.subgraph(sub_nodes)  # CrÃ©e un sous-graphe rÃ©duit
else:
    G_reduit = G

nodes = list(G_reduit.nodes())  # Liste des nÅ“uds du sous-graphe

# DÃ©finition des variables de dÃ©cision
x = pulp.LpVariable.dicts('x', nodes, cat=pulp.LpBinary)

# DÃ©finition du problÃ¨me d'optimisation
problem = pulp.LpProblem('MDS', pulp.LpMinimize)

# Fonction objectif : minimiser le nombre de nÅ“uds sÃ©lectionnÃ©s
problem += pulp.lpSum([x[i] for i in nodes])

# Contrainte : chaque nÅ“ud doit Ãªtre dominÃ© (lui-mÃªme ou un voisin)
added_constraints = set()
for i in nodes:
    neighbors_i = tuple(sorted(set(G_reduit.neighbors(i)) | {i}))  # Trie et enlÃ¨ve les doublons
    if neighbors_i not in added_constraints:  # VÃ©rifie si la contrainte existe dÃ©jÃ 
        problem += pulp.lpSum([x[n] for n in neighbors_i]) >= 1
        added_constraints.add(neighbors_i)  # Ajoute la contrainte Ã  l'ensemble

# Utilisation du solveur CBC
cbc_path = r"C:\Users\lydia\Downloads\Cbc-releases.2.10.12-windows-2022-msvs-v17-Release-x64\bin\cbc.exe"

from pulp import COIN_CMD

#solver = COIN_CMD(path=cbc_path, msg=True, keepFiles=True)
status = problem.solve(pulp.PULP_CBC_CMD())

# Extraction du Minimum Dominating Set (MDS)
dominating_set = [i for i in nodes if pulp.value(x[i]) == 1]
st.write(f"âœ… Nombre de bibliothÃ¨ques sÃ©lectionnÃ©es pour couvrir tout le rÃ©seau : {len(dominating_set)}")
st.write(f"ðŸ“Œ BibliothÃ¨ques sÃ©lectionnÃ©es : {dominating_set[:10]}...")  # Affiche seulement 10 bibliothÃ¨ques pour Ã©viter la surcharge

# Visualisation du MDS sur le graphe
pos = nx.spring_layout(G_reduit, k=0.3)

plt.figure(figsize=(12, 8))
nx.draw_networkx_edges(G_reduit, pos, alpha=0.3, edge_color="gray")
nx.draw_networkx_nodes(G_reduit, pos, node_size=50, node_color="blue", alpha=0.5)
nx.draw_networkx_nodes(G_reduit, pos, nodelist=dominating_set, node_size=100, node_color="red", alpha=1)
plt.title("Minimum Dominating Set Connexe")

# Affichage avec Streamlit
st.pyplot(plt)


# ðŸ“Œ Charger une partie des donnÃ©es pour Ã©viter la surcharge mÃ©moire
fichier_csv = "data/bibliotheques.csv"
df = pd.read_csv(fichier_csv, encoding="utf-8", delimiter=";", nrows=1000)  # Charger uniquement les 1000 premiÃ¨res lignes

# ðŸ“Œ Nettoyage des donnÃ©es
df = df.dropna(subset=["Latitude", "Longitude", "Surface", "Nombre de bÃ©nÃ©voles"])

# ðŸ“Œ CrÃ©ation de la carte interactive avec Folium
st.title("ðŸ“š Analyse des BibliothÃ¨ques en France")
st.write("Carte interactive des bibliothÃ¨ques et analyse du rÃ©seau de connexions.")

map_center = [df["Latitude"].mean(), df["Longitude"].mean()]
mymap = folium.Map(location=map_center, zoom_start=6)
marker_cluster = MarkerCluster().add_to(mymap)

for _, row in df.iterrows():
    folium.Marker(
        location=[row["Latitude"], row["Longitude"]],
        popup=f"{row['Nom de l\'Ã©tablissement']} - {row['Ville']} ({row['Surface']} mÂ²)",
        icon=folium.Icon(color="blue", icon="book"),
    ).add_to(marker_cluster)

st_folium(mymap)  # ðŸ“Œ Affichage de la carte interactive

# ðŸ“Œ Graphique interactif des bibliothÃ¨ques par rÃ©gion
df_counts = df["RÃ©gion"].value_counts()
fig = px.bar(df_counts, x=df_counts.index, y=df_counts.values, labels={'x': "RÃ©gion", 'y': "Nombre de BibliothÃ¨ques"}, title="ðŸ“Š Nombre de BibliothÃ¨ques par RÃ©gion")
fig.update_layout(showlegend=False)
st.plotly_chart(fig)

# ðŸ“Œ Ajout de filtres interactifs
region = st.selectbox("ðŸ“ Choisir une rÃ©gion :", df["RÃ©gion"].unique())
surface_min, surface_max = st.slider("ðŸ“ Filtrer par surface (mÂ²) :", int(df["Surface"].min()), int(df["Surface"].max()), (500, 5000))

# ðŸ“Œ Filtrer selon la sÃ©lection
df_filtered = df[(df["RÃ©gion"] == region) & (df["Surface"].between(surface_min, surface_max))]
st.write(f"ðŸ“Œ BibliothÃ¨ques en {region} entre {surface_min} et {surface_max} mÂ² :")
st.dataframe(df_filtered)

# ðŸ“Œ CrÃ©ation du graphe des bibliothÃ¨ques
st.subheader("ðŸ”— RÃ©seau des BibliothÃ¨ques")
G = nx.Graph()
for _, row in df.iterrows():
    G.add_node(row["Nom de l'Ã©tablissement"], ville=row["Ville"], latitude=row["Latitude"], longitude=row["Longitude"])

seuil_distance_km = st.sidebar.slider("ðŸ” Distance max entre bibliothÃ¨ques (km)", 10, 200, 50, 10)
for i, biblio1 in enumerate(G.nodes(data=True)):
    for j, biblio2 in enumerate(G.nodes(data=True)):
        if i >= j:
            continue
        coord1 = (biblio1[1]["latitude"], biblio1[1]["longitude"])
        coord2 = (biblio2[1]["latitude"], biblio2[1]["longitude"])
        distance = haversine(coord1, coord2)
        if distance < seuil_distance_km:
            G.add_edge(biblio1[0], biblio2[0], weight=distance)

st.sidebar.write(f"ðŸ“Œ Nombre total de connexions : {G.number_of_edges()}")

# ðŸ“Œ RÃ©seau interactif avec Pyvis
st.subheader("ðŸ“Œ RÃ©seau interactif des bibliothÃ¨ques")
net = Network(height="600px", width="100%", notebook=False)
for node, data in G.nodes(data=True):
    net.add_node(node, label=node)

for edge in G.edges():
    net.add_edge(edge[0], edge[1])

net.save_graph("graph.html")
st.components.v1.html(open("assets/graph.html", "r", encoding="utf-8").read(), height=600)

st.success("ðŸš€ Application interactive terminÃ©e ! Explorez les bibliothÃ¨ques et connexions en temps rÃ©el.")
